{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da704da-93c4-4bba-9a37-a2d3567c2a6b",
   "metadata": {},
   "source": [
    "# Modeling on Roseman Stored Data\n",
    "\n",
    "In this notebook, we will build predictive models based on the cleaned and processed Roseman Stored dataset.\n",
    "\n",
    "## Objectives\n",
    "- Select appropriate features for modeling.\n",
    "- Split the data into training and testing sets.\n",
    "- Train various machine learning models.\n",
    "- Evaluate the models' performance.\n",
    "- Draw insights and recommendations based on the model results.\n",
    "\n",
    "*Note:* This notebook follows the data cleaning and visualization steps done previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28f5e2",
   "metadata": {},
   "source": [
    "### Importing essential libraries for regression modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aac6357-8d3d-4b83-970e-db49ae9d8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Model evaluation metrics for regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Optional: for saving/loading models\n",
    "import joblib\n",
    "\n",
    "# Optional: warnings control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deee2fc-5827-45e0-a4d9-b29b34c767a9",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34b2eb7-c3d2-452b-a36c-707649825110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_overview(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Show basic structure of the DataFrame:\n",
    "    - Shape\n",
    "    - Column names\n",
    "    - Data types and non-null values\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Overview of {name} ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\n--- Info ---\")\n",
    "    print(df.info())\n",
    "\n",
    "\n",
    "def data_statistics(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Show statistical summary of the DataFrame:\n",
    "    - Descriptive statistics for all columns\n",
    "    - Number of unique values per column\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Descriptive Statistics of {name} ---\")\n",
    "    print(df.describe(include='all').transpose())\n",
    "    \n",
    "    print(f\"\\n--- Unique Values per Column in {name} ---\")\n",
    "    print(df.nunique().sort_values())\n",
    "\n",
    "\n",
    "def missing_values_report(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Display a formatted text report of missing values in the DataFrame.\n",
    "    The output looks like a table, but it's printed as plain text.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Missing Values in {name} ---\")\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_percent = (missing_count / len(df)) * 100\n",
    "\n",
    "    # Keep only columns with missing values\n",
    "    mask = missing_count > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"No missing values detected.\")\n",
    "        return\n",
    "\n",
    "    # Print table header\n",
    "    print(f\"{'':<18}{'Missing Count':>15}  {'Missing %':>10}\")\n",
    "\n",
    "    # Print each row aligned\n",
    "    for col in df.columns[mask]:\n",
    "        count = missing_count[col]\n",
    "        percent = missing_percent[col]\n",
    "        print(f\"{col:<18}{count:>15}  {percent:>10.6f}\")\n",
    "\n",
    "\n",
    "def show_value_counts(df, columns):\n",
    "    \"\"\"\n",
    "    Display value counts for a list of columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame\n",
    "    columns (list): A list of column names for which to show value counts\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        print(f\"--- Value Counts for column: '{col}' ---\")\n",
    "        print(df[col].value_counts())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def train_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Train the given model on training data, predict on both training and test data,\n",
    "    then calculate and print MAE, RMSE, and R2 metrics for both datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: sklearn-compatible regression model instance\n",
    "    - x_train: training features\n",
    "    - y_train: training targets\n",
    "    - x_test: test features\n",
    "    - y_test: test targets\n",
    "    \"\"\"\n",
    "    model.fit(x_train, y_train)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"{model.__class__.__name__} Performance Metrics\\n\")\n",
    "    print(\"Training Set:\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae_train:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse_train:.4f}\")\n",
    "    print(f\"  R-squared Score (R2): {r2_train:.4f}\\n\")\n",
    "    print(\"Test Set:\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae_test:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse_test:.4f}\")\n",
    "    print(f\"  R-squared Score (R2): {r2_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025987e-6c38-4682-8249-f2068ec13338",
   "metadata": {},
   "source": [
    "### Load Cleaned Datasets for Further Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72a9a26-8ddf-4f3f-acbd-2ecfbe174c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned training data\n",
    "clean_data = pd.read_csv(r\"clean_data.csv\")\n",
    "\n",
    "# Load raw test data\n",
    "test_data = pd.read_csv(r\"test_data.csv\")\n",
    "\n",
    "# Merge both datasets to apply unified preprocessing\n",
    "df = pd.concat([clean_data, test_data], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bdf7c",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Dropping Irrelevant Columns and Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e72972e-44ee-42b0-b321-d5d6c010e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overview of Full Data ---\n",
      "Shape: (1058297, 23)\n",
      "Columns: ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'compdistance', 'compmonth', 'compyear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'is_test', 'Year', 'Month', 'Day', 'Day_name']\n",
      "\n",
      "--- Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1058297 entries, 0 to 1058296\n",
      "Data columns (total 23 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   Store            1058297 non-null  int64  \n",
      " 1   DayOfWeek        1058297 non-null  int64  \n",
      " 2   Date             1058297 non-null  object \n",
      " 3   Sales            1017209 non-null  float64\n",
      " 4   Customers        1017209 non-null  float64\n",
      " 5   Open             1058297 non-null  float64\n",
      " 6   Promo            1058297 non-null  int64  \n",
      " 7   StateHoliday     1058297 non-null  object \n",
      " 8   SchoolHoliday    1058297 non-null  int64  \n",
      " 9   StoreType        1058297 non-null  object \n",
      " 10  Assortment       1058297 non-null  object \n",
      " 11  compdistance     1058297 non-null  float64\n",
      " 12  compmonth        1058297 non-null  float64\n",
      " 13  compyear         1058297 non-null  float64\n",
      " 14  Promo2           1058297 non-null  int64  \n",
      " 15  Promo2SinceWeek  1058297 non-null  float64\n",
      " 16  Promo2SinceYear  1058297 non-null  float64\n",
      " 17  PromoInterval    1058297 non-null  object \n",
      " 18  is_test          1058297 non-null  bool   \n",
      " 19  Year             1058297 non-null  int64  \n",
      " 20  Month            1058297 non-null  int64  \n",
      " 21  Day              1058297 non-null  int64  \n",
      " 22  Day_name         1058297 non-null  object \n",
      "dtypes: bool(1), float64(8), int64(8), object(6)\n",
      "memory usage: 178.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use data overview function to display basic info about the cleaned data\n",
    "data_overview(df, name=\"Full Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5b400b-df2c-483f-aa85-a3da00adfe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Descriptive Statistics of Full Data ---\n",
      "                     count unique         top     freq         mean  \\\n",
      "Store            1058297.0    NaN         NaN      NaN   558.331493   \n",
      "DayOfWeek        1058297.0    NaN         NaN      NaN     3.997596   \n",
      "Date               1058297    990  2015-07-31     1115          NaN   \n",
      "Sales            1017209.0    NaN         NaN      NaN  5773.818972   \n",
      "Customers        1017209.0    NaN         NaN      NaN    -1.364282   \n",
      "Open             1058297.0    NaN         NaN      NaN     0.831048   \n",
      "Promo            1058297.0    NaN         NaN      NaN      0.38207   \n",
      "StateHoliday       1058297      5           0   625531          NaN   \n",
      "SchoolHoliday    1058297.0    NaN         NaN      NaN     0.188929   \n",
      "StoreType          1058297      4           a   573755          NaN   \n",
      "Assortment         1058297      3           a   557749          NaN   \n",
      "compdistance     1058297.0    NaN         NaN      NaN    -0.048868   \n",
      "compmonth        1058297.0    NaN         NaN      NaN     7.466895   \n",
      "compyear         1058297.0    NaN         NaN      NaN  2009.108061   \n",
      "Promo2           1058297.0    NaN         NaN      NaN     0.503671   \n",
      "Promo2SinceWeek  1058297.0    NaN         NaN      NaN    11.746069   \n",
      "Promo2SinceYear  1058297.0    NaN         NaN      NaN  1013.264002   \n",
      "PromoInterval      1058297      4     noPromo   525263          NaN   \n",
      "is_test            1058297      2       False  1017209          NaN   \n",
      "Year             1058297.0    NaN         NaN      NaN  2013.877628   \n",
      "Month            1058297.0    NaN         NaN      NaN     5.944111   \n",
      "Day              1058297.0    NaN         NaN      NaN    15.618076   \n",
      "Day_name           1058297      7    Thursday   151837          NaN   \n",
      "\n",
      "                         std       min       25%     50%       75%       max  \n",
      "Store             321.845581       1.0     280.0   558.0     837.0    1115.0  \n",
      "DayOfWeek           1.998099       1.0       2.0     4.0       6.0       7.0  \n",
      "Date                     NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "Sales            3849.926175       0.0    3727.0  5744.0    7856.0   41551.0  \n",
      "Customers            3.43204  -8.85024 -0.561785     0.0  0.438215  3.441989  \n",
      "Open                0.374709       0.0       1.0     1.0       1.0       1.0  \n",
      "Promo               0.485894       0.0       0.0     0.0       1.0       1.0  \n",
      "StateHoliday             NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "SchoolHoliday       0.391452       0.0       0.0     0.0       0.0       1.0  \n",
      "StoreType                NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "Assortment               NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "compdistance        0.685828 -2.076153 -0.523446     0.0  0.476554  1.535275  \n",
      "compmonth           2.671983       1.0       6.0     8.0       9.0      12.0  \n",
      "compyear            5.007062    1900.0    2008.0  2010.0    2011.0    2015.0  \n",
      "Promo2              0.499987       0.0       0.0     1.0       1.0       1.0  \n",
      "Promo2SinceWeek    15.365765       0.0       0.0     1.0      22.0      50.0  \n",
      "Promo2SinceYear  1005.851963       0.0       0.0  2009.0    2012.0    2015.0  \n",
      "PromoInterval            NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "is_test                  NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "Year                0.794836    2013.0    2013.0  2014.0    2015.0    2015.0  \n",
      "Month               3.298015       1.0       3.0     6.0       9.0      12.0  \n",
      "Day                 8.784827       1.0       8.0    16.0      23.0      31.0  \n",
      "Day_name                 NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "\n",
      "--- Unique Values per Column in Full Data ---\n",
      "SchoolHoliday          2\n",
      "Promo2                 2\n",
      "is_test                2\n",
      "Open                   2\n",
      "Promo                  2\n",
      "Assortment             3\n",
      "Year                   3\n",
      "StoreType              4\n",
      "PromoInterval          4\n",
      "StateHoliday           5\n",
      "DayOfWeek              7\n",
      "Day_name               7\n",
      "Promo2SinceYear        8\n",
      "Month                 12\n",
      "compmonth             12\n",
      "compyear              23\n",
      "Promo2SinceWeek       25\n",
      "Day                   31\n",
      "compdistance         654\n",
      "Date                 990\n",
      "Store               1115\n",
      "Customers           4086\n",
      "Sales              21734\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use data statistics function to show descriptive stats and unique values\n",
    "data_statistics(df, name=\"Full Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3126fc",
   "metadata": {},
   "source": [
    "### Preprocessing: Dropping & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c598827f-e73a-43f4-b881-3630635abc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Missing Values in Full Data ---\n",
      "                    Missing Count   Missing %\n",
      "Sales                       41088    3.882464\n",
      "Customers                   41088    3.882464\n"
     ]
    }
   ],
   "source": [
    "# Generate a missing values report for the cleaned dataset\n",
    "missing_values_report(df, name=\"Full Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416be92a-a694-44c7-9b35-1e43e1b7cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Date', 'DayOfWeek' and is_test columns because their information is already captured by 'Year', 'Month', 'Day', and other features.\n",
    "# Keeping these columns would be redundant.\n",
    "columns_to_drop = ['Date', 'DayOfWeek', 'is_test']\n",
    "\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1218d42-5e7e-42e2-850b-f804b824fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Value Counts for column: 'StateHoliday' ---\n",
      "StateHoliday\n",
      "0    625531\n",
      "0    401536\n",
      "a     20440\n",
      "b      6690\n",
      "c      4100\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "--- Value Counts for column: 'StoreType' ---\n",
      "StoreType\n",
      "a    573755\n",
      "d    327024\n",
      "c    141112\n",
      "b     16406\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "--- Value Counts for column: 'Assortment' ---\n",
      "Assortment\n",
      "a    557749\n",
      "c    491822\n",
      "b      8726\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "--- Value Counts for column: 'PromoInterval' ---\n",
      "PromoInterval\n",
      "noPromo             525263\n",
      "Jan,Apr,Jul,Oct     306898\n",
      "Feb,May,Aug,Nov     124308\n",
      "Mar,Jun,Sept,Dec    101828\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "--- Value Counts for column: 'Day_name' ---\n",
      "Day_name\n",
      "Thursday     151837\n",
      "Wednesday    151657\n",
      "Tuesday      151656\n",
      "Friday       150981\n",
      "Monday       150722\n",
      "Sunday       150722\n",
      "Saturday     150722\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract columns with categorical data type\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Generate value counts report for categorical columns\n",
    "show_value_counts(df, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4fea8-c7e2-40ec-9b2d-147a8bfe87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map StateHoliday categorical values to numerical codes\n",
    "stateholiday_map ={'0': 0, 'a': 1, 'b': 2, 'c': 3}\n",
    "df['StateHoliday'] = df['StateHoliday'].map(stateholiday_map)\n",
    "\n",
    "# Map Assortment categorical values to numerical codes\n",
    "assortment_map = {'a': 0, 'b': 1, 'c': 2}\n",
    "df['Assortment'] = df['Assortment'].map(assortment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c572dde3-7146-4bbb-a3d0-53a6ee26a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to 'Day_name', 'StoreType' and 'PromoInterval' columns\n",
    "# - dtype='int64' ensures resulting dummy variables are integers, which saves memory and is often preferred for ML models\n",
    "# - drop_first=True avoids dummy variable trap (multicollinearity) by dropping the first category from each encoded column\n",
    "df = pd.get_dummies(df, columns=['StoreType', 'PromoInterval', 'Day_name'], dtype='int64', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2afc5649-1c04-45e2-b692-fa76806752ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between each feature and 'Sales':\n",
      "\n",
      "Sales                             1.000000\n",
      "Customers                         0.767544\n",
      "Open                              0.678472\n",
      "Promo                             0.452345\n",
      "Day_name_Monday                   0.215309\n",
      "StoreType_b                       0.139940\n",
      "Day_name_Tuesday                  0.130764\n",
      "PromoInterval_noPromo             0.091040\n",
      "SchoolHoliday                     0.085124\n",
      "Day_name_Wednesday                0.083047\n",
      "Assortment                        0.074941\n",
      "Day_name_Thursday                 0.050344\n",
      "Month                             0.048768\n",
      "Year                              0.023519\n",
      "compyear                          0.010115\n",
      "Day_name_Saturday                 0.007801\n",
      "Store                             0.005126\n",
      "StoreType_c                      -0.005140\n",
      "Day                              -0.011612\n",
      "StoreType_d                      -0.022854\n",
      "compmonth                        -0.023638\n",
      "PromoInterval_Jan,Apr,Jul,Oct    -0.034946\n",
      "Promo2SinceWeek                  -0.044143\n",
      "PromoInterval_Mar,Jun,Sept,Dec   -0.053267\n",
      "compdistance                     -0.056210\n",
      "Promo2                           -0.091040\n",
      "Promo2SinceYear                  -0.091056\n",
      "StateHoliday                     -0.283471\n",
      "Day_name_Sunday                  -0.589219\n",
      "Name: Sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation between each feature and 'Sales'\n",
    "sales_correlations = df.corr(numeric_only=True)['Sales'].sort_values(ascending=False)\n",
    "\n",
    "# Display correlations\n",
    "print(\"Correlation between each feature and 'Sales':\\n\")\n",
    "print(sales_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d35d315-5ef9-47a4-8fc6-a8d77eecfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Customers' column because it is missing in test data \n",
    "# and is strongly correlated with the target variable 'Sales',\n",
    "# which makes it redundant and potentially misleading for model training.\n",
    "df.drop(columns=['Customers'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c640b23a-5aa9-4068-9da6-58f32e3507c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets based on missing values in 'Sales'\n",
    "# Rows with non-missing 'Sales' will be used for training\n",
    "# Rows with missing 'Sales' will be used for testing (unlabeled data)\n",
    "train_data = df[df['Sales'].notnull()].copy()\n",
    "test_data = df[df['Sales'].isnull()].copy()\n",
    "\n",
    "# Optional: reset index for both sets\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf8ce661-0150-4dc1-bc2a-febde5728c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Missing Values in Train Dataset ---\n",
      "                    Missing Count   Missing %\n",
      "StateHoliday               393216   38.656363\n",
      "\n",
      "--- Missing Values in Test Dataset ---\n",
      "                    Missing Count   Missing %\n",
      "Sales                       41088  100.000000\n",
      "StateHoliday                 8320   20.249221\n"
     ]
    }
   ],
   "source": [
    "# Generate a missing values report for the train and test dataset\n",
    "missing_values_report(train_data, name=\"Train Dataset\")\n",
    "missing_values_report(test_data, name=\"Test Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e281f",
   "metadata": {},
   "source": [
    "### Data Splitting, Scaling & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d88f5b00-1dac-40e7-9316-465215bd0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "x = train_data.drop('Sales', axis=1)  # Drop the target column from the features\n",
    "y = train_data['Sales']  # Extract the target column\n",
    "\n",
    "# Split the data into training and testing sets (85% train, 15% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c966040-ec65-4c53-87c9-3e9d20d190eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to scale features for both training and testing sets\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform them\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# Transform the testing features using the same scaler fitted on training data\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3efff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.isnan(x_train_scaled).sum())  # nunmber of mussing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f26d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "x_train_scaled = imputer.fit_transform(x_train_scaled)\n",
    "x_test_scaled = imputer.transform(x_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cf72c92-b360-4d8c-ab2e-05e6f5aff1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction after StandardScaler\n",
    "pca = PCA(n_components=0.90)  # Keep 90% of variance\n",
    "\n",
    "# Fit PCA on scaled training data and transform it\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "\n",
    "# Transform the scaled testing data using the same PCA model\n",
    "x_test_pca = pca.transform(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d691ca-1a88-4b0a-ae61-75fd87f4976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train after PCA: (864627, 18)\n",
      "Shape of X_test after PCA: (152582, 18)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the PCA-transformed training and testing data\n",
    "print(\"Shape of X_train after PCA:\", x_train_pca.shape)\n",
    "print(\"Shape of X_test after PCA:\", x_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f305ca5-0ba9-4a02-a47c-ed62d549436a",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece6dda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in y_train: 0\n",
      "NaNs in y_test:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"NaNs in y_train:\", np.isnan(y_train).sum())\n",
    "print(\"NaNs in y_test: \", np.isnan(y_test).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd1c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression Performance Metrics\n",
      "\n",
      "Training Set:\n",
      "  Mean Absolute Error (MAE): 1747.0587\n",
      "  Root Mean Squared Error (RMSE): 2494.4812\n",
      "  R-squared Score (R2): 0.5804\n",
      "\n",
      "Test Set:\n",
      "  Mean Absolute Error (MAE): 1739.1423\n",
      "  Root Mean Squared Error (RMSE): 2483.2273\n",
      "  R-squared Score (R2): 0.5825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_train_i = imputer.fit_transform(x_train)\n",
    "x_test_i  = imputer.transform(x_test)\n",
    "\n",
    "train_evaluate_model(LinearRegression(), x_train_i, y_train, x_test_i, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "712bcf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in TRAIN per column:\n",
      "feat_0     864627\n",
      "feat_1     864627\n",
      "feat_2     864627\n",
      "feat_3     864627\n",
      "feat_4     864627\n",
      "feat_5     864627\n",
      "feat_6     864627\n",
      "feat_7     864627\n",
      "feat_8     864627\n",
      "feat_9     864627\n",
      "feat_10    864627\n",
      "feat_11    864627\n",
      "feat_12    864627\n",
      "feat_13    864627\n",
      "feat_14    864627\n",
      "feat_15    864627\n",
      "feat_16    864627\n",
      "feat_17    864627\n",
      "feat_18    864627\n",
      "feat_19    864627\n",
      "feat_20    864627\n",
      "feat_21    864627\n",
      "feat_22    864627\n",
      "feat_23    864627\n",
      "feat_24    864627\n",
      "feat_25    864627\n",
      "feat_26    864627\n",
      "dtype: int64\n",
      "\n",
      "Missing in TEST per column:\n",
      "feat_0     152582\n",
      "feat_1     152582\n",
      "feat_2     152582\n",
      "feat_3     152582\n",
      "feat_4     152582\n",
      "feat_5     152582\n",
      "feat_6     152582\n",
      "feat_7     152582\n",
      "feat_8     152582\n",
      "feat_9     152582\n",
      "feat_10    152582\n",
      "feat_11    152582\n",
      "feat_12    152582\n",
      "feat_13    152582\n",
      "feat_14    152582\n",
      "feat_15    152582\n",
      "feat_16    152582\n",
      "feat_17    152582\n",
      "feat_18    152582\n",
      "feat_19    152582\n",
      "feat_20    152582\n",
      "feat_21    152582\n",
      "feat_22    152582\n",
      "feat_23    152582\n",
      "feat_24    152582\n",
      "feat_25    152582\n",
      "feat_26    152582\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(x_train, columns=[f'feat_{i}' for i in range(x_train.shape[1])])\n",
    "df_test  = pd.DataFrame(x_test,  columns=[f'feat_{i}' for i in range(x_test.shape[1])])\n",
    "\n",
    "print(\"Missing in TRAIN per column:\")\n",
    "print(df_train.isna().sum())\n",
    "\n",
    "print(\"\\nMissing in TEST per column:\")\n",
    "print(df_test.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d86ec4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean') \n",
    "x_train = imputer.fit_transform(x_train)\n",
    "x_test = imputer.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc1eeae9-9be3-47b6-8c44-25191b07f2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression Performance Metrics\n",
      "\n",
      "Training Set:\n",
      "  Mean Absolute Error (MAE): 1747.0587\n",
      "  Root Mean Squared Error (RMSE): 2494.4812\n",
      "  R-squared Score (R2): 0.5804\n",
      "\n",
      "Test Set:\n",
      "  Mean Absolute Error (MAE): 1739.1423\n",
      "  Root Mean Squared Error (RMSE): 2483.2273\n",
      "  R-squared Score (R2): 0.5825\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model and evaluate it using the predefined function\n",
    "train_evaluate_model(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c621264-2292-4359-bd9d-8396786cbbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor Performance Metrics\n",
      "\n",
      "Training Set:\n",
      "  Mean Absolute Error (MAE): 774.4136\n",
      "  Root Mean Squared Error (RMSE): 1107.2430\n",
      "  R-squared Score (R2): 0.9173\n",
      "\n",
      "Test Set:\n",
      "  Mean Absolute Error (MAE): 780.3175\n",
      "  Root Mean Squared Error (RMSE): 1120.2096\n",
      "  R-squared Score (R2): 0.9150\n"
     ]
    }
   ],
   "source": [
    "# Initialize the XGBoost Regressor model\n",
    "model = XGBRegressor(random_state=42, use_label_encoder=False, eval_metric='rmse')\n",
    "\n",
    "# Train and evaluate using the reusable function\n",
    "train_evaluate_model(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7168d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "XGBRegressor Performance Metrics\n",
      "\n",
      "Training Set:\n",
      "  Mean Absolute Error (MAE): 358.6147\n",
      "  Root Mean Squared Error (RMSE): 544.4490\n",
      "  R-squared Score (R2): 0.9800\n",
      "\n",
      "Test Set:\n",
      "  Mean Absolute Error (MAE): 410.6212\n",
      "  Root Mean Squared Error (RMSE): 651.1451\n",
      "  R-squared Score (R2): 0.9713\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'reg_lambda': [0.5, 1, 1.5, 2],\n",
    "}\n",
    "\n",
    "# Randomized Search CV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, use_label_encoder=False, eval_metric='rmse'),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the search on training data\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best model from search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Train and evaluate using the provided function\n",
    "train_evaluate_model(best_model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983afc73",
   "metadata": {},
   "source": [
    "### Save the trained model, PCA transformer, and Standard Scaler for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ef98997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model, scaler, and PCA have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the best trained regression model to disk\n",
    "joblib.dump(best_model, 'XGB_Model.pkl')\n",
    "\n",
    "# Save the fitted StandardScaler to disk\n",
    "joblib.dump(scaler, 'standard_scaler.pkl')\n",
    "\n",
    "# Save the fitted PCA transformer to disk\n",
    "joblib.dump(pca, 'pca_transformer.pkl')\n",
    "\n",
    "print(\"Best model, scaler, and PCA have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26685a7",
   "metadata": {},
   "source": [
    "### Save final test data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8823992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Sales' column from the test_data DataFrame if it exists\n",
    "test_data = test_data.drop(columns=['Sales'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c94f80bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41088, 27)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "332292d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(864627, 27)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12ed6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  Open  Promo  StateHoliday  SchoolHoliday  Assortment  compdistance  \\\n",
      "0      1   1.0      1           0.0              0           0     -0.267367   \n",
      "1      3   1.0      1           0.0              0           0      0.794428   \n",
      "2      7   1.0      1           0.0              0           2      1.027952   \n",
      "3      8   1.0      1           0.0              0           0      0.516402   \n",
      "4      9   1.0      1           0.0              0           2     -0.060734   \n",
      "\n",
      "   compmonth  compyear  Promo2  ...  PromoInterval_Jan,Apr,Jul,Oct  \\\n",
      "0        9.0    2008.0       0  ...                              0   \n",
      "1       12.0    2006.0       1  ...                              1   \n",
      "2        4.0    2013.0       0  ...                              0   \n",
      "3       10.0    2014.0       0  ...                              0   \n",
      "4        8.0    2000.0       0  ...                              0   \n",
      "\n",
      "   PromoInterval_Mar,Jun,Sept,Dec  PromoInterval_noPromo  Day_name_Monday  \\\n",
      "0                               0                      1                0   \n",
      "1                               0                      0                0   \n",
      "2                               0                      1                0   \n",
      "3                               0                      1                0   \n",
      "4                               0                      1                0   \n",
      "\n",
      "   Day_name_Saturday  Day_name_Sunday  Day_name_Thursday  Day_name_Tuesday  \\\n",
      "0                  0                0                  1                 0   \n",
      "1                  0                0                  1                 0   \n",
      "2                  0                0                  1                 0   \n",
      "3                  0                0                  1                 0   \n",
      "4                  0                0                  1                 0   \n",
      "\n",
      "   Day_name_Wednesday        Sales  \n",
      "0                   0  5008.232910  \n",
      "1                   0  7763.977539  \n",
      "2                   0  9334.880859  \n",
      "3                   0  6202.517090  \n",
      "4                   0  7438.729492  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Add predictions as a new column in test_data\n",
    "test_data['Sales'] = predictions\n",
    "\n",
    "# Show updated DataFrame (optional)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c917d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the test data along with the predicted sales to a CSV file for future reporting or analysis\n",
    "test_data.to_csv('XGB_TestData_With_Predicted_Sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c19c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
