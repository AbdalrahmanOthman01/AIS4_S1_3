{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da704da-93c4-4bba-9a37-a2d3567c2a6b",
   "metadata": {},
   "source": [
    "# Modeling on Roseman Stored Data\n",
    "\n",
    "In this notebook, we will build predictive models based on the cleaned and processed Roseman Stored dataset.\n",
    "\n",
    "## Objectives\n",
    "- Select appropriate features for modeling.\n",
    "- Split the data into training and testing sets.\n",
    "- Train various machine learning models.\n",
    "- Evaluate the models' performance.\n",
    "- Draw insights and recommendations based on the model results.\n",
    "\n",
    "*Note:* This notebook follows the data cleaning and visualization steps done previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28f5e2",
   "metadata": {},
   "source": [
    "### Importing essential libraries for regression modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad373c31-790f-416c-8a84-8d2931c1f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9aac6357-8d3d-4b83-970e-db49ae9d8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Model evaluation metrics for regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Optional: for saving/loading models\n",
    "import joblib\n",
    "\n",
    "# Optional: warnings control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deee2fc-5827-45e0-a4d9-b29b34c767a9",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b34b2eb7-c3d2-452b-a36c-707649825110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_overview(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Show basic structure of the DataFrame:\n",
    "    - Shape\n",
    "    - Column names\n",
    "    - Data types and non-null values\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Overview of {name} ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\n--- Info ---\")\n",
    "    print(df.info())\n",
    "\n",
    "\n",
    "def data_statistics(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Show statistical summary of the DataFrame:\n",
    "    - Descriptive statistics for all columns\n",
    "    - Number of unique values per column\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Descriptive Statistics of {name} ---\")\n",
    "    print(df.describe(include='all').transpose())\n",
    "    \n",
    "    print(f\"\\n--- Unique Values per Column in {name} ---\")\n",
    "    print(df.nunique().sort_values())\n",
    "\n",
    "\n",
    "def missing_values_report(df, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Display a formatted text report of missing values in the DataFrame.\n",
    "    The output looks like a table, but it's printed as plain text.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Missing Values in {name} ---\")\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_percent = (missing_count / len(df)) * 100\n",
    "\n",
    "    # Keep only columns with missing values\n",
    "    mask = missing_count > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"No missing values detected.\")\n",
    "        return\n",
    "\n",
    "    # Print table header\n",
    "    print(f\"{'':<18}{'Missing Count':>15}  {'Missing %':>10}\")\n",
    "\n",
    "    # Print each row aligned\n",
    "    for col in df.columns[mask]:\n",
    "        count = missing_count[col]\n",
    "        percent = missing_percent[col]\n",
    "        print(f\"{col:<18}{count:>15}  {percent:>10.6f}\")\n",
    "\n",
    "\n",
    "def show_value_counts(df, columns):\n",
    "    \"\"\"\n",
    "    Display value counts for a list of columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame\n",
    "    columns (list): A list of column names for which to show value counts\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        print(f\"--- Value Counts for column: '{col}' ---\")\n",
    "        print(df[col].value_counts())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def train_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Train the given model on training data, predict on both training and test data,\n",
    "    then calculate and print MAE, RMSE, and R2 metrics for both datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: sklearn-compatible regression model instance\n",
    "    - x_train: training features\n",
    "    - y_train: training targets\n",
    "    - x_test: test features\n",
    "    - y_test: test targets\n",
    "    \"\"\"\n",
    "    model.fit(x_train, y_train)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"{model.__class__.__name__} Performance Metrics\\n\")\n",
    "    print(\"Training Set:\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae_train:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse_train:.4f}\")\n",
    "    print(f\"  R-squared Score (R2): {r2_train:.4f}\\n\")\n",
    "    print(\"Test Set:\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae_test:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse_test:.4f}\")\n",
    "    print(f\"  R-squared Score (R2): {r2_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025987e-6c38-4682-8249-f2068ec13338",
   "metadata": {},
   "source": [
    "### Load Cleaned Datasets for Further Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d72a9a26-8ddf-4f3f-acbd-2ecfbe174c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned training data\n",
    "clean_data = pd.read_csv(r\"D:\\Work\\CODING\\PYTHON\\DEPI-project\\data\\clean_data.csv\")\n",
    "\n",
    "# Load raw test data\n",
    "test_data = pd.read_csv(r\"D:\\Work\\CODING\\PYTHON\\DEPI-project\\data\\test_data.csv\")\n",
    "\n",
    "# Merge both datasets to apply unified preprocessing\n",
    "df = pd.concat([clean_data, test_data], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bdf7c",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Dropping Irrelevant Columns and Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e72972e-44ee-42b0-b321-d5d6c010e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overview of Full Data ---\n",
      "Shape: (1058297, 22)\n",
      "Columns: ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'compdistance', 'compmonth', 'compyear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Year', 'Month', 'Day', 'Day_name']\n",
      "\n",
      "--- Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1058297 entries, 0 to 1058296\n",
      "Data columns (total 22 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   Store            1058297 non-null  int64  \n",
      " 1   DayOfWeek        1058297 non-null  int64  \n",
      " 2   Date             1058297 non-null  object \n",
      " 3   Sales            1017209 non-null  float64\n",
      " 4   Customers        1058297 non-null  float64\n",
      " 5   Open             1058297 non-null  int64  \n",
      " 6   Promo            1058297 non-null  int64  \n",
      " 7   StateHoliday     1058297 non-null  object \n",
      " 8   SchoolHoliday    1058297 non-null  int64  \n",
      " 9   StoreType        1058297 non-null  object \n",
      " 10  Assortment       1058297 non-null  object \n",
      " 11  compdistance     1058297 non-null  float64\n",
      " 12  compmonth        1058297 non-null  float64\n",
      " 13  compyear         1058297 non-null  float64\n",
      " 14  Promo2           1058297 non-null  int64  \n",
      " 15  Promo2SinceWeek  1058297 non-null  float64\n",
      " 16  Promo2SinceYear  1058297 non-null  float64\n",
      " 17  PromoInterval    1058297 non-null  object \n",
      " 18  Year             1058297 non-null  int64  \n",
      " 19  Month            1058297 non-null  int64  \n",
      " 20  Day              1058297 non-null  int64  \n",
      " 21  Day_name         1058297 non-null  object \n",
      "dtypes: float64(7), int64(9), object(6)\n",
      "memory usage: 177.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use data overview function to display basic info about the cleaned data\n",
    "data_overview(df, name=\"Full Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f5b400b-df2c-483f-aa85-a3da00adfe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Descriptive Statistics of Full Data ---\n",
      "                     count unique         top    freq         mean  \\\n",
      "Store            1058297.0    NaN         NaN     NaN   558.331493   \n",
      "DayOfWeek        1058297.0    NaN         NaN     NaN     3.997596   \n",
      "Date               1058297    990  2015-07-31    1115          NaN   \n",
      "Sales            1017209.0    NaN         NaN     NaN  5773.818972   \n",
      "Customers        1058297.0    NaN         NaN     NaN    -1.435253   \n",
      "Open             1058297.0    NaN         NaN     NaN     0.831048   \n",
      "Promo            1058297.0    NaN         NaN     NaN      0.38207   \n",
      "StateHoliday       1058297      5           0  625531          NaN   \n",
      "SchoolHoliday    1058297.0    NaN         NaN     NaN     0.188929   \n",
      "StoreType          1058297      4           a  573755          NaN   \n",
      "Assortment         1058297      3           a  557749          NaN   \n",
      "compdistance     1058297.0    NaN         NaN     NaN    -0.048868   \n",
      "compmonth        1058297.0    NaN         NaN     NaN     7.466895   \n",
      "compyear         1058297.0    NaN         NaN     NaN  2009.108061   \n",
      "Promo2           1058297.0    NaN         NaN     NaN     0.503671   \n",
      "Promo2SinceWeek  1058297.0    NaN         NaN     NaN    11.746069   \n",
      "Promo2SinceYear  1058297.0    NaN         NaN     NaN  1013.264002   \n",
      "PromoInterval      1058297      4     noPromo  525263          NaN   \n",
      "Year             1058297.0    NaN         NaN     NaN  2013.877628   \n",
      "Month            1058297.0    NaN         NaN     NaN     5.944111   \n",
      "Day              1058297.0    NaN         NaN     NaN    15.618076   \n",
      "Day_name           1058297      7    Thursday  151837          NaN   \n",
      "\n",
      "                         std       min       25%     50%       75%       max  \n",
      "Store             321.845581       1.0     280.0   558.0     837.0    1115.0  \n",
      "DayOfWeek           1.998099       1.0       2.0     4.0       6.0       7.0  \n",
      "Date                     NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "Sales            3849.926175       0.0    3727.0  5744.0    7856.0   41551.0  \n",
      "Customers           3.243563 -7.807787 -0.600574     0.0  0.399426  3.078826  \n",
      "Open                0.374709       0.0       1.0     1.0       1.0       1.0  \n",
      "Promo               0.485894       0.0       0.0     0.0       1.0       1.0  \n",
      "StateHoliday             NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "SchoolHoliday       0.391452       0.0       0.0     0.0       0.0       1.0  \n",
      "StoreType                NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "Assortment               NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "compdistance        0.685828 -2.076153 -0.523446     0.0  0.476554  1.535275  \n",
      "compmonth           2.671983       1.0       6.0     8.0       9.0      12.0  \n",
      "compyear            5.007062    1900.0    2008.0  2010.0    2011.0    2015.0  \n",
      "Promo2              0.499987       0.0       0.0     1.0       1.0       1.0  \n",
      "Promo2SinceWeek    15.365765       0.0       0.0     1.0      22.0      50.0  \n",
      "Promo2SinceYear  1005.851963       0.0       0.0  2009.0    2012.0    2015.0  \n",
      "PromoInterval            NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "Year                0.794836    2013.0    2013.0  2014.0    2015.0    2015.0  \n",
      "Month               3.298015       1.0       3.0     6.0       9.0      12.0  \n",
      "Day                 8.784827       1.0       8.0    16.0      23.0      31.0  \n",
      "Day_name                 NaN       NaN       NaN     NaN       NaN       NaN  \n",
      "\n",
      "--- Unique Values per Column in Full Data ---\n",
      "Promo2                 2\n",
      "Open                   2\n",
      "Promo                  2\n",
      "SchoolHoliday          2\n",
      "Assortment             3\n",
      "Year                   3\n",
      "PromoInterval          4\n",
      "StoreType              4\n",
      "StateHoliday           5\n",
      "Day_name               7\n",
      "DayOfWeek              7\n",
      "Promo2SinceYear        8\n",
      "compmonth             12\n",
      "Month                 12\n",
      "compyear              23\n",
      "Promo2SinceWeek       25\n",
      "Day                   31\n",
      "compdistance         654\n",
      "Date                 990\n",
      "Store               1115\n",
      "Customers           4086\n",
      "Sales              21734\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use data statistics function to show descriptive stats and unique values\n",
    "data_statistics(df, name=\"Full Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3126fc",
   "metadata": {},
   "source": [
    "### Preprocessing: Dropping & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c598827f-e73a-43f4-b881-3630635abc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Missing Values in Full Data ---\n",
      "                    Missing Count   Missing %\n",
      "Sales                       41088    3.882464\n"
     ]
    }
   ],
   "source": [
    "# Generate a missing values report for the cleaned dataset\n",
    "missing_values_report(df, name=\"Full Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "416be92a-a694-44c7-9b35-1e43e1b7cf92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['is_test'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Drop 'Date', 'DayOfWeek' and is_test columns because their information is already captured by 'Year', 'Month', 'Day', and other features.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Keeping these columns would be redundant.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDayOfWeek\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcolumns_to_drop, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['is_test'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop 'Date', 'DayOfWeek' and is_test columns because their information is already captured by 'Year', 'Month', 'Day', and other features.\n",
    "# Keeping these columns would be redundant.\n",
    "columns_to_drop = ['Date', 'DayOfWeek', 'is_test']\n",
    "\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1218d42-5e7e-42e2-850b-f804b824fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns with categorical data type\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Generate value counts report for categorical columns\n",
    "show_value_counts(df, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4fea8-c7e2-40ec-9b2d-147a8bfe87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map StateHoliday categorical values to numerical codes\n",
    "stateholiday_map = {'0': 0, 'a': 1, 'b': 2, 'c': 3}\n",
    "df['StateHoliday'] = df['StateHoliday'].map(stateholiday_map)\n",
    "\n",
    "# Map Assortment categorical values to numerical codes\n",
    "assortment_map = {'a': 0, 'b': 1, 'c': 2}\n",
    "df['Assortment'] = df['Assortment'].map(assortment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572dde3-7146-4bbb-a3d0-53a6ee26a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to 'Day_name', 'StoreType' and 'PromoInterval' columns\n",
    "# - dtype='int64' ensures resulting dummy variables are integers, which saves memory and is often preferred for ML models\n",
    "# - drop_first=True avoids dummy variable trap (multicollinearity) by dropping the first category from each encoded column\n",
    "df = pd.get_dummies(df, columns=['StoreType', 'PromoInterval', 'Day_name'], dtype='int64', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc5649-1c04-45e2-b692-fa76806752ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between each feature and 'Sales'\n",
    "sales_correlations = df.corr(numeric_only=True)['Sales'].sort_values(ascending=False)\n",
    "\n",
    "# Display correlations\n",
    "print(\"Correlation between each feature and 'Sales':\\n\")\n",
    "print(sales_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35d315-5ef9-47a4-8fc6-a8d77eecfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Customers' column because it is missing in test data \n",
    "# and is strongly correlated with the target variable 'Sales',\n",
    "# which makes it redundant and potentially misleading for model training.\n",
    "df.drop(columns=['Customers'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640b23a-5aa9-4068-9da6-58f32e3507c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets based on missing values in 'Sales'\n",
    "# Rows with non-missing 'Sales' will be used for training\n",
    "# Rows with missing 'Sales' will be used for testing (unlabeled data)\n",
    "train_data = df[df['Sales'].notnull()].copy()\n",
    "test_data = df[df['Sales'].isnull()].copy()\n",
    "\n",
    "# Optional: reset index for both sets\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ce661-0150-4dc1-bc2a-febde5728c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a missing values report for the train and test dataset\n",
    "missing_values_report(train_data, name=\"Train Dataset\")\n",
    "missing_values_report(test_data, name=\"Test Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e281f",
   "metadata": {},
   "source": [
    "### Data Splitting, Scaling & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d88f5b00-1dac-40e7-9316-465215bd0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "x = train_data.drop('Sales', axis=1)  # Drop the target column from the features\n",
    "y = train_data['Sales']  # Extract the target column\n",
    "\n",
    "# Split the data into training and testing sets (85% train, 15% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c966040-ec65-4c53-87c9-3e9d20d190eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to scale features for both training and testing sets\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform them\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# Transform the testing features using the same scaler fitted on training data\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cf72c92-b360-4d8c-ab2e-05e6f5aff1f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m)  \u001b[38;5;66;03m# Keep 90% of variance\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit PCA on scaled training data and transform it\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m x_train_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(x_train_scaled)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Transform the scaled testing data using the same PCA model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m x_test_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(x_test_scaled)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:511\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    512\u001b[0m     X,\n\u001b[0;32m    513\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m[xp\u001b[38;5;241m.\u001b[39mfloat64, xp\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[0;32m    514\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    515\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    516\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    517\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    518\u001b[0m )\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    124\u001b[0m     X,\n\u001b[0;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Apply PCA for dimensionality reduction after StandardScaler\n",
    "pca = PCA(n_components=0.90)  # Keep 90% of variance\n",
    "\n",
    "# Fit PCA on scaled training data and transform it\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "\n",
    "# Transform the scaled testing data using the same PCA model\n",
    "x_test_pca = pca.transform(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31d691ca-1a88-4b0a-ae61-75fd87f4976d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check the shape of the PCA-transformed training and testing data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X_train after PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_train_pca\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X_test after PCA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_test_pca\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_pca' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the shape of the PCA-transformed training and testing data\n",
    "print(\"Shape of X_train after PCA:\", x_train_pca.shape)\n",
    "print(\"Shape of X_test after PCA:\", x_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f305ca5-0ba9-4a02-a47c-ed62d549436a",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc1eeae9-9be3-47b6-8c44-25191b07f2c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model and evaluate it using the predefined function\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_evaluate_model(model, x_train, y_train, x_test, y_test)\n",
      "Cell \u001b[1;32mIn[68], line 79\u001b[0m, in \u001b[0;36mtrain_evaluate_model\u001b[1;34m(model, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_evaluate_model\u001b[39m(model, x_train, y_train, x_test, y_test):\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    Train the given model on training data, predict on both training and test data,\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    then calculate and print MAE, RMSE, and R2 metrics for both datasets.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    - y_test: test targets\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m     80\u001b[0m     y_train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_train)\n\u001b[0;32m     81\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:609\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    605\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    607\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 609\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    610\u001b[0m     X,\n\u001b[0;32m    611\u001b[0m     y,\n\u001b[0;32m    612\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m    613\u001b[0m     y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    614\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    615\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    616\u001b[0m )\n\u001b[0;32m    618\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1304\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1305\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1306\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1307\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1308\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1309\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1310\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1311\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1312\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1313\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1314\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    124\u001b[0m     X,\n\u001b[0;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model and evaluate it using the predefined function\n",
    "train_evaluate_model(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c621264-2292-4359-bd9d-8396786cbbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor Performance Metrics\n",
      "\n",
      "Training Set:\n",
      "  Mean Absolute Error (MAE): 787.0561\n",
      "  Root Mean Squared Error (RMSE): 1125.3441\n",
      "  R-squared Score (R2): 0.9146\n",
      "\n",
      "Test Set:\n",
      "  Mean Absolute Error (MAE): 785.2866\n",
      "  Root Mean Squared Error (RMSE): 1123.5722\n",
      "  R-squared Score (R2): 0.9145\n"
     ]
    }
   ],
   "source": [
    "# Initialize the XGBoost Regressor model\n",
    "model = XGBRegressor(random_state=42, use_label_encoder=False, eval_metric='rmse')\n",
    "\n",
    "# Train and evaluate using the reusable function\n",
    "train_evaluate_model(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7168d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "XGBRegressor Performance Metrics\n",
      "\n",
      "Training Set:\n",
      "  Mean Absolute Error (MAE): 359.6804\n",
      "  Root Mean Squared Error (RMSE): 546.8981\n",
      "  R-squared Score (R2): 0.9798\n",
      "\n",
      "Test Set:\n",
      "  Mean Absolute Error (MAE): 406.8700\n",
      "  Root Mean Squared Error (RMSE): 639.5243\n",
      "  R-squared Score (R2): 0.9723\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'reg_lambda': [0.5, 1, 1.5, 2],\n",
    "}\n",
    "\n",
    "# Randomized Search CV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, use_label_encoder=False, eval_metric='rmse'),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the search on training data\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best model from search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Train and evaluate using the provided function\n",
    "train_evaluate_model(best_model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983afc73",
   "metadata": {},
   "source": [
    "### Save the trained model, PCA transformer, and Standard Scaler for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef98997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model, scaler, and PCA have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the best trained regression model to disk\n",
    "joblib.dump(best_model, 'XGB_Model.pkl')\n",
    "\n",
    "# Save the fitted StandardScaler to disk\n",
    "joblib.dump(scaler, 'standard_scaler.pkl')\n",
    "\n",
    "# Save the fitted PCA transformer to disk\n",
    "joblib.dump(pca, 'pca_transformer.pkl')\n",
    "\n",
    "print(\"Best model, scaler, and PCA have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26685a7",
   "metadata": {},
   "source": [
    "### Save final test data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8823992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Sales' column from the test_data DataFrame if it exists\n",
    "test_data = test_data.drop(columns=['Sales'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c94f80bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41088, 27)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "332292d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(864627, 27)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12ed6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  Open  Promo  StateHoliday  SchoolHoliday  Assortment  compdistance  \\\n",
      "0      1   1.0      1             0              0           0        1270.0   \n",
      "1      1   1.0      1             0              0           0        1270.0   \n",
      "2      1   1.0      1             0              0           0        1270.0   \n",
      "3      1   1.0      1             0              0           0        1270.0   \n",
      "4      1   0.0      0             0              0           0        1270.0   \n",
      "\n",
      "   compmonth  compyear  Promo2  ...  PromoInterval_Jan,Apr,Jul,Oct  \\\n",
      "0        9.0    2008.0       0  ...                              0   \n",
      "1        9.0    2008.0       0  ...                              0   \n",
      "2        9.0    2008.0       0  ...                              0   \n",
      "3        9.0    2008.0       0  ...                              0   \n",
      "4        9.0    2008.0       0  ...                              0   \n",
      "\n",
      "   PromoInterval_Mar,Jun,Sept,Dec  PromoInterval_noPromo  Day_name_Monday  \\\n",
      "0                               0                      1                0   \n",
      "1                               0                      1                0   \n",
      "2                               0                      1                0   \n",
      "3                               0                      1                1   \n",
      "4                               0                      1                0   \n",
      "\n",
      "   Day_name_Saturday  Day_name_Sunday  Day_name_Thursday  Day_name_Tuesday  \\\n",
      "0                  0                0                  1                 0   \n",
      "1                  0                0                  0                 0   \n",
      "2                  0                0                  0                 1   \n",
      "3                  0                0                  0                 0   \n",
      "4                  0                1                  0                 0   \n",
      "\n",
      "   Day_name_Wednesday        Sales  \n",
      "0                   0  5366.602051  \n",
      "1                   1  5481.147949  \n",
      "2                   0  6043.616699  \n",
      "3                   0  7628.304688  \n",
      "4                   0  -526.923218  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Add predictions as a new column in test_data\n",
    "test_data['Sales'] = predictions\n",
    "\n",
    "# Show updated DataFrame (optional)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the test data along with the predicted sales to a CSV file for future reporting or analysis\n",
    "test_data.to_csv('XGB_TestData_With_Predicted_Sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c19c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
